train:
    # optimizer
    learning_rate: 0.0001
    max_grad_norm: 50

    # a3c
    agents_n: 32
    update_agent_frequency: 40
    sample_entropy: false
    sample_lr: false

    # loss
    gamma: 0.99 # train discount coef
    tau: 1.00 # GAE parameter
    value_weight: 0.5
    entropy_weight: 0.001

    # intrinsic rewards
    curiosity_weight: 0.001

    # auxiliary tasks
    use_pixel_control: true
    use_reward_prediction: true
    use_value_replay: true
    pc_coef: 0.001
    rp_coef: 0.1

    # backups
    save_frequency: 5

environment:
    env_type: "dmlab"
    env_name: "lt_horseshoe_color"
    skip_frames: 0
    clip_rewards: true
    stack_frames: 1
    frame_h: 84
    frame_w: 84
    normalize_env: true
    noop_max: null
    fps: 15
    episode_length_sec: 120
    bot_count: 4 # [0, 6]
    bot_skill: 1 # [1, 5]
